# Deep Learnig Projects
# ğŸ§  AI Deep Learning Practice - TÃ¢che 1 : RÃ©gression avec Keras

## ğŸ¯ Objectif
Cette tÃ¢che vise Ã  construire un modÃ¨le de rÃ©gression utilisant un **MLP (Multilayer Perceptron)** avec **Keras (TensorFlow backend)** afin de prÃ©dire une variable continue, comme le prix des maisons.

---

## ğŸ“‚ TÃ¢che 1 : RÃ©gression supervisÃ©e

### ğŸ“Œ Description
Utilisation du dataset **California Housing** pour entraÃ®ner un rÃ©seau de neurones simple Ã  estimer une valeur continue Ã  partir de donnÃ©es tabulaires.

---

## ğŸ› ï¸ Outils & Technologies

- Python 3.x
- TensorFlow / Keras
- Scikit-learn
- Matplotlib / Seaborn

---

## ğŸ“Š Dataset

- **California Housing** : Dataset accessible via `sklearn.datasets.fetch_california_housing()`

---

## âš™ï¸ Ã‰tapes du projet

### 1. Chargement et exploration des donnÃ©es
- Import du dataset
- SÃ©paration features / target

### 2. PrÃ©traitement
- Division en jeu d'entraÃ®nement/test (80/20)
- Normalisation des donnÃ©es avec `StandardScaler`

### 3. ModÃ©lisation avec Keras
- ModÃ¨le MLP : 
  - Dense(64, activation='relu')
  - Dense(32, activation='relu')
  - Dense(1) (sortie)
- Optimiseur : `adam`
- Fonction de coÃ»t : `mse`
- MÃ©triques : `mae`

### 4. EntraÃ®nement
- Validation split (20%)
- Epochs : 100
- Batch size : 32

### 5. Ã‰valuation
- MÃ©triques calculÃ©es :
  - RMSE (Root Mean Squared Error)
  - MAE (Mean Absolute Error)
- Visualisation de la courbe d'apprentissage

---

âœ… **But commun de ces mÃ©triques** : Ã‰valuer la capacitÃ© du modÃ¨le Ã  **faire des prÃ©dictions proches des valeurs rÃ©elles**. Plus les scores sont bas, meilleur est le modÃ¨le.


---

## ğŸ“ˆ RÃ©sultats attendus

- Affichage des scores MAE, MSE, RMSE sur le jeu de test
- Graphe de la perte d'entraÃ®nement/validation au fil des epochs

---

## ğŸ“Š MÃ©triques utilisÃ©es

### ğŸ”¹ MSE (Mean Squared Error)
- **DÃ©finition** : Moyenne des carrÃ©s des Ã©carts entre les prÃ©dictions du modÃ¨le et les vraies valeurs.
- **Objectif** : PÃ©naliser fortement les erreurs importantes (car lâ€™erreur est Ã©levÃ©e au carrÃ©).
- **UtilitÃ©** : Utile pour dÃ©tecter les modÃ¨les qui ont tendance Ã  faire de grosses erreurs sur certains Ã©chantillons.

### ğŸ”¹ RMSE (Root Mean Squared Error)
- **DÃ©finition** : Racine carrÃ©e du MSE.
- **Objectif** : Fournir une mesure dâ€™erreur dans la mÃªme unitÃ© que la variable prÃ©dite (par exemple, en milliers de dollars pour le prix des maisons).
- **UtilitÃ©** : InterprÃ©table directement sur lâ€™Ã©chelle de la cible. Plus facile Ã  comparer Ã  une valeur rÃ©elle moyenne.

### ğŸ”¹ MAE (Mean Absolute Error)
- **DÃ©finition** : Moyenne des valeurs absolues des Ã©carts entre les prÃ©dictions et les vraies valeurs.
- **Objectif** : Mesurer lâ€™erreur moyenne sans donner trop dâ€™importance aux erreurs extrÃªmes.
- **UtilitÃ©** : Plus robuste que le MSE en prÃ©sence dâ€™outliers (valeurs aberrantes).

---

âœ… **But commun de ces mÃ©triques** : Ã‰valuer la capacitÃ© du modÃ¨le Ã  **faire des prÃ©dictions proches des valeurs rÃ©elles**. Plus les scores sont bas, meilleur est le modÃ¨le.

---
---

## ğŸ“‚ TÃ¢che 2 : Classification supervisÃ©e

### ğŸ“Œ Description
Utilisation du dataset **MNIST** pour entraÃ®ner un modÃ¨le de classification dâ€™images Ã  base de convolution. Lâ€™objectif est de reconnaÃ®tre automatiquement les chiffres manuscrits Ã  partir dâ€™images en niveaux de gris (28x28 pixels).

---

## ğŸ¯ Objectif
Cette tÃ¢che consiste Ã  construire un **rÃ©seau de neurones convolutif (CNN)** avec **PyTorch** afin de classer des images de chiffres manuscrits du dataset **MNIST** en 10 classes (de 0 Ã  9).

---

## ğŸ› ï¸ Outils & Technologies

- Python 3.x
- PyTorch
- Torchvision
- Matplotlib
- Scikit-learn

---

## ğŸ“Š Dataset

- **MNIST** : Dataset intÃ©grÃ© Ã  `torchvision.datasets.MNIST`
  - 60 000 images dâ€™entraÃ®nement
  - 10 000 images de test
  - 10 classes (chiffres de 0 Ã  9)
  - Taille : 28x28 pixels, monochrome

---

## âš™ï¸ Ã‰tapes du projet

### 1. Chargement des donnÃ©es
- Import du dataset via `torchvision`
- Normalisation des images

### 2. PrÃ©traitement
- CrÃ©ation des `DataLoader` pour lâ€™entraÃ®nement et le test
- Batch size = 64 pour le train, 1000 pour le test

### 3. ModÃ©lisation avec PyTorch

Dans cette Ã©tape, on construit un **rÃ©seau de neurones convolutifs (CNN)** Ã  lâ€™aide de la bibliothÃ¨que `torch.nn`. Ce type dâ€™architecture est particuliÃ¨rement efficace pour les donnÃ©es image, car il capture les **caractÃ©ristiques spatiales** (bords, textures, formes...) Ã  travers des couches de convolution.

#### ğŸ§± Architecture du modÃ¨le

Le modÃ¨le utilisÃ© pour MNIST se compose de :

- **Couche 1 : Convolution 2D**
  - `nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)`
  - ReÃ§oit une image 28Ã—28 en niveaux de gris (1 canal)
  - Produit 16 cartes de caractÃ©ristiques (feature maps) de mÃªme taille grÃ¢ce au padding
  - **Activation : ReLU**
  - **RÃ©duction spatiale : MaxPooling (2x2)** â†’ taille devient 14Ã—14

- **Couche 2 : Convolution 2D**
  - `nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)`
  - Prend en entrÃ©e les 16 feature maps de la couche prÃ©cÃ©dente
  - Produit 32 cartes â†’ taille encore 14Ã—14 â†’ MaxPooling rÃ©duit Ã  7Ã—7
  - **Activation : ReLU**
  - **MaxPooling (2x2)**

- **Flatten**
  - Aplatissement de la sortie des convolutions : `32 * 7 * 7 = 1568` neurones

- **Couche Fully Connected (dense)**
  - `nn.Linear(in_features=1568, out_features=128)`
  - **Activation : ReLU**

- **Sortie (Classification)**
  - `nn.Linear(in_features=128, out_features=10)`
  - Donne un vecteur de 10 scores (logits), un pour chaque chiffre (0 Ã  9)

#### ğŸ§  Fonctions clÃ©s

- **Activation : ReLU (Rectified Linear Unit)**
  - Permet au rÃ©seau dâ€™apprendre des relations non linÃ©aires
  - Aide Ã  Ã©viter le problÃ¨me du "vanishing gradient"

- **Fonction de perte : `nn.CrossEntropyLoss`**
  - Combine `LogSoftmax` + `Negative Log Likelihood`
  - AdaptÃ©e Ã  la **classification multi-classes**
  - Calcule la diffÃ©rence entre les vraies classes et les probabilitÃ©s prÃ©dites

- **Optimiseur : `torch.optim.Adam`**
  - MÃ©thode dâ€™optimisation adaptative
  - Ajuste dynamiquement le learning rate de chaque paramÃ¨tre
  - Efficace, stable, et largement utilisÃ© pour l'entraÃ®nement de modÃ¨les profonds

#### ğŸ§® Dimensions rÃ©sumÃ©es Ã  chaque Ã©tape (entrÃ©e = 28Ã—28)
| Ã‰tape | Taille sortie | Canaux |
|------|---------------|--------|
| Conv1 + Pooling | 14Ã—14     | 16     |
| Conv2 + Pooling | 7Ã—7       | 32     |
| Flatten         | 1568      | â€”      |
| FC1             | 128       | â€”      |
| FC2 (Sortie)    | 10        | â€”      |

---

ğŸ“Œ Remarque : Le modÃ¨le reste volontairement **simple et lÃ©ger**, adaptÃ© Ã  un petit dataset comme MNIST pour un entraÃ®nement rapide tout en illustrant les bases de la **convolution**, **pooling**, **flatten**, et **dense layers**.


### 4. EntraÃ®nement
- Boucle dâ€™entraÃ®nement sur plusieurs epochs
- Affichage de la perte par epoch

### 5. Ã‰valuation
- PrÃ©dictions sur le jeu de test
- Calcul de lâ€™accuracy globale
- GÃ©nÃ©ration de la matrice de confusion

---


## ğŸ“Š MÃ©triques utilisÃ©es

### ğŸ”¹ Accuracy
- **DÃ©finition** : Proportion de prÃ©dictions correctes sur lâ€™ensemble des Ã©chantillons.
- **Objectif** : Ã‰valuer la prÃ©cision globale du modÃ¨le de classification.
- **UtilitÃ©** : TrÃ¨s adaptÃ©e aux datasets Ã©quilibrÃ©s comme MNIST.

### ğŸ”¹ Matrice de confusion
- **DÃ©finition** : Tableau croisÃ© indiquant les bonnes et mauvaises prÃ©dictions pour chaque classe.
- **Objectif** : Identifier quelles classes sont confondues entre elles.
- **UtilitÃ©** : Utile pour analyser les erreurs spÃ©cifiques (ex. : 4 prÃ©dits comme 9).

---

âœ… **But commun de ces mÃ©triques** : Ã‰valuer la capacitÃ© du modÃ¨le Ã  **reconnaÃ®tre correctement les images** et Ã  **rÃ©duire les confusions entre classes**. Une haute accuracy et une matrice de confusion bien diagonale indiquent un bon modÃ¨le.

---

## â„¹ï¸ Fonction dâ€™activation utilisÃ©e : ReLU

- **DÃ©finition** : `ReLU(x) = max(0, x)`
- **Objectif** : Ajouter de la non-linÃ©aritÃ© au rÃ©seau pour lui permettre dâ€™apprendre des fonctions complexes.
- **Avantage** : Simple, rapide, et efficace pour les rÃ©seaux profonds. Elle Ã©vite le problÃ¨me du gradient qui disparaÃ®t.

---
## ğŸ§© Visualisation du modÃ¨le avec `torchviz`

Afin de mieux comprendre la structure du modÃ¨le CNN construit avec PyTorch, il est possible de **gÃ©nÃ©rer un graphe visuel** du flux computationnel (calcul des sorties Ã  partir des entrÃ©es). Cela aide Ã  observer les **connexions entre les couches** et la faÃ§on dont les donnÃ©es traversent le rÃ©seau.

### âœ… Ã‰tapes pour afficher le graphe dans Google Colab

### 1. Installer la bibliothÃ¨que `torchviz`
```python
!pip install torchviz

---

## ğŸ“‚ TÃ¢che 3 : Classification multi-classes

### ğŸ“Œ Description
Utilisation du dataset **Fashion-MNIST** pour entraÃ®ner un modÃ¨le de classification dâ€™images avec un **CNN construit avec TensorFlow/Keras**. Lâ€™objectif est de reconnaÃ®tre automatiquement des objets ou chiffres manuscrits Ã  partir dâ€™images.

---

## ğŸ¯ Objectif
Concevoir un **rÃ©seau convolutif robuste avec rÃ©gularisation et data augmentation** capable de distinguer plusieurs classes (0 Ã  9), en utilisant **Keras** (backend TensorFlow).

---

## ğŸ› ï¸ Outils & Technologies

- Python 3.x
- TensorFlow / Keras
- Matplotlib / Seaborn
- Scikit-learn

---

## ğŸ“Š Dataset

- **Fashion MNIST** (vÃªtements)
  - 60 000 images d'entraÃ®nement, 10 000 de test
  - Images 28x28, en niveaux de gris
  - 10 classes
  - Accessible via `tensorflow.keras.datasets`

---

## âš™ï¸ Ã‰tapes du projet

### 1. PrÃ©traitement des donnÃ©es
- Chargement via `keras.datasets.fashion_mnist`
- Normalisation des pixels `[0, 255] â†’ [0, 1]`
- Reshape des images au format (28, 28, 1)
- Encodage one-hot des labels (`to_categorical`)

### 2. Construction du modÃ¨le CNN
- `Conv2D(32)` + ReLU + MaxPooling + Dropout
- `Conv2D(64)` + ReLU + MaxPooling + Dropout
- `Flatten` â†’ `Dense(128)` + ReLU + Dropout
- `Dense(10)` avec activation `softmax` (sortie)

### 3. Compilation du modÃ¨le
- Optimiseur : `Adam`
- Fonction de perte : `categorical_crossentropy`
- MÃ©trique : `accuracy`

### 4. EntraÃ®nement du modÃ¨le
Deux variantes sont testÃ©es :
- **Sans Data Augmentation**
- **Avec Data Augmentation** (`ImageDataGenerator` : rotation, shift, zoom...)

### 5. Ã‰valuation
- `evaluate()` sur le jeu de test
- Matrice de confusion
- Rapport de classification (`precision`, `recall`, `f1-score`)
- Courbes de prÃ©cision par epoch

---

## ğŸ“ˆ RÃ©sultats comparÃ©s

| Variante               | Accuracy (test) | F1-score moyen | Classe 6 (F1) |
|------------------------|----------------|----------------|---------------|
| âŒ Sans augmentation   | 89 %           | 0.89           | 0.69          |
| âœ… Avec augmentation   | 89 %           | 0.89           | 0.69          |

â¡ï¸ **Observation** : Les rÃ©sultats sont trÃ¨s proches, mais lâ€™usage de la data augmentation permet une meilleure **gÃ©nÃ©ralisation**, en particulier pour les classes sous-reprÃ©sentÃ©es ou confondues (comme la classe 6).

---

## ğŸ“Š Rapport de classification (extraits)

**Sans Data Augmentation** :
accuracy: 0.89
macro avg F1: 0.89
classe 6 F1: 0.69

**Avec Data Augmentation** :
accuracy: 0.89
macro avg F1: 0.89
classe 6 F1: 0.69

## ğŸ Data Augmentation

Ajout de transformations artificielles via `ImageDataGenerator` pour augmenter la robustesse du modÃ¨le :

```python
from tensorflow.keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(
    rotation_range=10,
    zoom_range=0.1,
    width_shift_range=0.1,
    height_shift_range=0.1
)

datagen.fit(x_train)

---

## Fonctions utilisÃ©es
`ReLU` : activation non linÃ©aire aprÃ¨s chaque convolution

`Softmax` : activation de sortie pour la classification multi-classes

`Dropout` : couche de rÃ©gularisation pour Ã©viter le surapprentissage

`Data Augmentation` : technique pour amÃ©liorer la gÃ©nÃ©ralisation du modÃ¨le en simulant des variations rÃ©alistes dâ€™images

